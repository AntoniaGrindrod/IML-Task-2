{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2e81b29c71eb2b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 2\n",
    "This serves as a template which will guide you through the implementation of this task. It is advised to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps.\n",
    "This is the jupyter notebook version of the template. For the python file version, please refer to the file `template_solution.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de347e31d213bd5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First, we import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e071b8e282a8d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-10T18:47:37.485752Z",
     "start_time": "2024-03-10T18:47:37.479263Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Add any other imports you need here\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, Matern, RationalQuadratic, WhiteKernel\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from fancyimpute import SoftImpute, MatrixFactorization\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f2086e18dd7b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Loading\n",
    "TODO: Perform data preprocessing, imputation and extract X_train, y_train and X_test\n",
    "(and potentially change initialization of variables to accomodate how you deal with non-numeric data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402e111cb0d70236",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "Shape: (900, 11)\n",
      "   season  price_AUS  price_CHF  price_CZE  price_GER  price_ESP  price_FRA  \\\n",
      "0  spring        NaN   9.644028  -1.686248  -1.748076  -3.666005        NaN   \n",
      "1  summer        NaN   7.246061  -2.132377  -2.054363  -3.295697  -4.104759   \n",
      "\n",
      "   price_UK  price_ITA  price_POL  price_SVK  \n",
      "0 -1.822720  -3.931031        NaN  -3.238197  \n",
      "1 -1.826021        NaN        NaN  -3.212894  \n",
      "\n",
      "\n",
      "Test data:\n",
      "(100, 10)\n",
      "   season  price_AUS  price_CZE  price_GER  price_ESP  price_FRA  price_UK  \\\n",
      "0  spring        NaN   0.472985   0.707957        NaN  -1.136441 -0.596703   \n",
      "1  summer  -1.184837   0.358019        NaN  -3.199028  -1.069695       NaN   \n",
      "\n",
      "   price_ITA  price_POL  price_SVK  \n",
      "0        NaN   3.298693   1.921886  \n",
      "1  -1.420091   3.238307        NaN  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_df_transposed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m\n\u001b[1;32m     38\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtest_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspring\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummer\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautumn\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinter\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m3.0\u001b[39m})\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#knn:\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#train_df_transposed = train_df.transpose()\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#train_df_knn = KNN(k=4).fit_transform(train_df_transposed)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#soft impute: \u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m train_df_si \u001b[38;5;241m=\u001b[39m SoftImpute()\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mtrain_df_transposed\u001b[49m)\n\u001b[1;32m     51\u001b[0m train_df_si \u001b[38;5;241m=\u001b[39m train_df_si\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m     53\u001b[0m test_df_si \u001b[38;5;241m=\u001b[39m SoftImpute()\u001b[38;5;241m.\u001b[39mfit_transform(test_df_transposed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df_transposed' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This loads the training and test data, preprocesses it, removes the NaN\n",
    "values and interpolates the missing data using imputation\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "Compute\n",
    "----------\n",
    "X_train: matrix of floats, training input with features\n",
    "y_train: array of floats, training output with labels\n",
    "X_test: matrix of floats: dim = (100, ?), test input with features\n",
    "\"\"\"\n",
    "# Load training data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "    \n",
    "print(\"Training data:\")\n",
    "print(\"Shape:\", train_df.shape)\n",
    "print(train_df.head(2))\n",
    "print('\\n')\n",
    "    \n",
    "# Load test data\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(\"Test data:\")\n",
    "print(test_df.shape)\n",
    "print(test_df.head(2))\n",
    "\n",
    "# Dummy initialization of the X_train, X_test and y_train   \n",
    "# TODO: Depending on how you deal with the non-numeric data, you may want to \n",
    "# modify/ignore the initialization of these variables   \n",
    "\n",
    "X_train = np.zeros_like(train_df.drop(['price_CHF'],axis=1))\n",
    "y_train = np.zeros_like(train_df['price_CHF'])\n",
    "X_test = np.zeros_like(test_df)\n",
    "\n",
    "# TODO: Perform data preprocessing, imputation and extract X_train, y_train and X_test\n",
    "train_df['season']=train_df['season'].map({'spring':0.0, 'summer':1.0, 'autumn':2.0, 'winter':3.0})\n",
    "test_df['season']=test_df['season'].map({'spring':0.0, 'summer':1.0, 'autumn':2.0, 'winter':3.0})\n",
    "\n",
    "#knn:\n",
    "#train_df_transposed = train_df.transpose()\n",
    "#train_df_knn = KNN(k=4).fit_transform(train_df_transposed)\n",
    "#train_df_knn = train_df_knn.transpose()\n",
    "\n",
    "#test_df_transposed = test_df.transpose()\n",
    "#test_df_knn = KNN(k=4).fit_transform(test_df_transposed)\n",
    "#test_df_knn = test_df_knn.transpose()\n",
    "\n",
    "#soft impute: \n",
    "train_df_si = SoftImpute().fit_transform(train_df_transposed)\n",
    "train_df_si = train_df_si.transpose()\n",
    "\n",
    "test_df_si = SoftImpute().fit_transform(test_df_transposed)\n",
    "test_df_si = test_df_si.transpose()\n",
    "\n",
    "#iterative imputer: \n",
    "it_imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "it_imp.fit(train_df)\n",
    "train_df_it_imp = it_imp.transform(train_df)\n",
    "\n",
    "it_imp.fit(test_df)\n",
    "test_df_it_imp = it_imp.transform(test_df)\n",
    "\n",
    "#matrix factorisation: \n",
    "mf = MatrixFactorization()\n",
    "train_df_mf = mf.fit_transform(train_df)\n",
    "test_df_mf = mf.fit_transform(test_df)\n",
    "\n",
    "assert (X_train.shape[1] == X_test.shape[1]) and (X_train.shape[0] == y_train.shape[0]) and (X_test.shape[0] == 100), \"Invalid data shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959037466887e870",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Modeling and Prediction\n",
    "TODO: Define the model and fit it using training data. Then, use test data to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de183f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This defines the model, fits training data and then does the prediction\n",
    "with the test data \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X_train: matrix of floats, training input with 10 features\n",
    "y_train: array of floats, training output\n",
    "X_test: matrix of floats: dim = (100, ?), test input with 10 features\n",
    "\n",
    "Compute\n",
    "----------\n",
    "y_test: array of floats: dim = (100,), predictions on test set\n",
    "\"\"\"\n",
    "\n",
    "full_train = train_df_it_imp #choose here one of: knn, si, it_imp or mf \n",
    "X_test = test_df_it_imp #as above \n",
    "\n",
    "X_train = np.delete(full_train, 2, axis = 1) #deleting Switzerland\n",
    "y_train = full_train[:, 2] #this is switzerland\n",
    "y_pred=np.zeros(X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59291352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: Define the model and fit it using training data. Then, use test data to make predictions\n",
    "gpr = GaussianProcessRegressor(kernel=DotProduct())\n",
    "gpr.fit(X_train, y_train)\n",
    "y_pred_dp = gpr.predict(X_test)\n",
    "\n",
    "#RBF: matern is a generalization of RBF so maybe skip RBF? \n",
    "#gpr = GaussianProcessRegressor(kernel=RBF(length_scale=0.1))\n",
    "#gpr.fit(X_train, y_train)\n",
    "#y_pred_rbf = gpr.predict(X_test)\n",
    "\n",
    "#Matern\n",
    "gpr = GaussianProcessRegressor(kernel=Matern(length_scale=0.1))\n",
    "gpr.fit(X_train, y_train)\n",
    "y_pred_mat = gpr.predict(X_test)\n",
    "\n",
    "#RationalQuadratic\n",
    "gpr = GaussianProcessRegressor(kernel=RationalQuadratic())\n",
    "gpr.fit(X_train, y_train)\n",
    "y_pred_rq = gpr.predict(X_test)\n",
    "\n",
    "\n",
    "#plotting \n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "axs[0].plot(y_pred_dp)\n",
    "axs[0].set_title('Dot product')\n",
    "axs[1].plot(y_pred_mat)\n",
    "axs[1].set_title('Matern')\n",
    "axs[2].plot(y_pred_rq)\n",
    "axs[2].set_title('Rational Quadr')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "assert y_pred.shape == (100,), \"Invalid data shape\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4caae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my combination\n",
    "gpr = GaussianProcessRegressor(kernel=RationalQuadratic()*Matern() + WhiteKernel())\n",
    "gpr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using the Gaussian process model\n",
    "y_pred, y_norm = gpr.predict(X_test,return_std=True)\n",
    "\n",
    "\n",
    "def expected_improvement(x, gp_model, best_y):\n",
    "    y_pred, y_std = gp_model.predict(x, return_std=True)\n",
    "    z = (y_pred - best_y) / y_std\n",
    "    ei = (y_pred - best_y) * norm.cdf(z) + y_std * norm.pdf(z)\n",
    "    return ei\n",
    "\n",
    "# Determine the point with the highest observed function value\n",
    "best_idx = np.argmax(y_train)\n",
    "best_x = X_train[best_idx]\n",
    "best_y = y_train[best_idx]\n",
    "\n",
    "ei = expected_improvement(X_train, gpr, best_y)\n",
    "\n",
    "def probability_of_improvement(x, gp_model, best_y):\n",
    "    y_pred, y_std = gp_model.predict(x, return_std=True)\n",
    "    z = (y_pred - best_y) / y_std\n",
    "    pi = norm.cdf(z)\n",
    "    return pi\n",
    "\n",
    "# Probability of Improvement\n",
    "pi = probability_of_improvement(X_train, gpr, best_y)\n",
    "\n",
    "num_iterations = 20\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Fit the Gaussian process model to the sampled points\n",
    "    gpr.fit(X_train, y_train)\n",
    "\n",
    "    # Determine the point with the highest observed function value\n",
    "    best_idx = np.argmax(y_train)\n",
    "    best_x = X_train[best_idx]\n",
    "    best_y = y_train[best_idx]\n",
    "\n",
    "    # Set the value of beta for the UCB acquisition function\n",
    "    beta = 2.0\n",
    "\n",
    "    # Generate the Upper Confidence Bound (UCB) using the Gaussian process model\n",
    "    #ucb = upper_confidence_bound(X_train, gpr, beta)\n",
    "\n",
    "y_pred=np.zeros(X_test.shape[0])\n",
    "#TODO: Define the model and fit it using training data. Then, use test data to make predictions\n",
    "\n",
    "y_pred = gpr.predict(X_test)\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(0,X_test.shape[0]),\n",
    "    y_pred,\n",
    "    label=\"Gaussian Process\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"dashdot\",\n",
    ")\n",
    "\n",
    "assert y_pred.shape == (100,), \"Invalid data shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c62e0cd4cec5a7e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Saving Results\n",
    "You don't have to change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d87d2d67ddbdc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = pd.DataFrame(y_pred) \n",
    "dt.columns = ['price_CHF']\n",
    "dt.to_csv('results.csv', index=False)\n",
    "print(\"\\nResults file successfully generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c9a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
